<h1 align="center"> Data Modeling with Apache Cassandra  </h1> <h2 align="center"> Data Engineering Nanodegree - Udacity </h2>


## Project Description
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analysis team is particularly interested in understanding what songs users are listening to. Currently, there is no easy way to query the data to generate the results, since the data reside in a directory of CSV files on user activity on the app.

In this project, created an Apache Cassandra database which can create queries on song play data to answer the questions and completed an ETL pipeline using Python. Created ETL pipeline that transfers data from a set of CSV files within a directory to create a streamlined CSV file to model and insert data into Apache Cassandra tables.


///will be update below
## Dataset
- Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song.

Sample Song Data:
```
{
    "num_songs":1,

}
```
- Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.



## Database Schema 
<img src="./dbdiagram.png" alt="Logo">

## Project Structure

|  File / Folder   |                         Description                          |
| :--------------: | :----------------------------------------------------------: |
|    data.zip      |               All song and log data JSONS                    |
| create_tables.py |                  Drops and creates tables                    |
|  sql_queries.py  |            SQL queries for data modeling and ETL             |
|    etl.ipynb     | Processes a file from song_data and log_data and loads the data into tables |
|      etl.py      | Processes all files from song_data and log_data and loads them into  tables |
|    test.ipynb    |                Explores the database tables                 |


## How to Run Python Scripts

To create the database tables and run the ETL pipeline, you must run the following two files in the order that they are listed below

To create tables:
```
python create_tables.py
```
To fill tables via ETL:
```
python etl.py
```
To check whether the data has been loaded into database by executing queries:
```
test.ipynb
```
Run ```create_tables.py``` file to reset tables before each time run your ETL scripts.

## Author
Burcu Belen - [Linkedin](https://www.linkedin.com/in/burcu-belen/)
